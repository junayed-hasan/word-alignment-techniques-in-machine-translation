import optparse
import sys
from collections import defaultdict

# Add the new parameter for number of iterations
optparser = optparse.OptionParser()
optparser.add_option("-d", "--data", dest="train", default="data/hansards", help="Data filename prefix (default=data)")
optparser.add_option("-e", "--english", dest="english", default="e", help="Suffix of English filename (default=e)")
optparser.add_option("-f", "--french", dest="french", default="f", help="Suffix of French filename (default=f)")
optparser.add_option("-n", "--num_sentences", dest="num_sents", default=100000000000, type="int", help="Number of sentences to use for training and alignment")
optparser.add_option("-i", "--num_iterations", dest="num_iterations", default=5, type="int", help="Number of iterations for training IBM Model 1")
(opts, _) = optparser.parse_args()

f_data = "%s.%s" % (opts.train, opts.french)
e_data = "%s.%s" % (opts.train, opts.english)

sys.stderr.write("Training with IBM Model 1...")
bitext = [[sentence.strip().split() for sentence in pair] for pair in zip(open(f_data), open(e_data))][:opts.num_sents]

# Get vocabularies
f_vocab = set()
e_vocab = set()
for (f, e) in bitext:
    f_vocab.update(f)
    e_vocab.update(e)

# Initialize translation probabilities
t = defaultdict(lambda: 1.0 / len(e_vocab))

# Train IBM Model 1
for iteration in range(opts.num_iterations):
    sys.stderr.write(f"Iteration {iteration + 1}...")
    count = defaultdict(float)
    total = defaultdict(float)

    for (f, e) in bitext:
        for f_word in f:
            Z = sum(t[(f_word, e_word)] for e_word in e)
            for e_word in e:
                c = t[(f_word, e_word)] / Z
                count[(f_word, e_word)] += c
                total[e_word] += c

    for (f_word, e_word) in count.keys():
        t[(f_word, e_word)] = count[(f_word, e_word)] / total[e_word]

    sys.stderr.write("done\n")

sys.stderr.write("Aligning...\n")

# Function to align a sentence pair
def align_sentence(f, e):
    alignment = []
    for i, f_word in enumerate(f):
        best_prob = 0
        best_j = 0
        for j, e_word in enumerate(e):
            if t[(f_word, e_word)] > best_prob:
                best_prob = t[(f_word, e_word)]
                best_j = j
        alignment.append(f"{i}-{best_j}")
    return " ".join(alignment)

# Align and output
for (f, e) in bitext:
    alignment = align_sentence(f, e)
    sys.stdout.write(f"{alignment}\n")

sys.stderr.write("\n")